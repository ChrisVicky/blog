<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>KinectV2 Camera Calibration and `Yolov5` Recognition | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera">
<meta property="og:type" content="article">
<meta property="og:title" content="KinectV2 Camera Calibration and &#96;Yolov5&#96; Recognition">
<meta property="og:url" content="http://example.com/2024/10/15/2022-12-30-KinectV2-Camera-Calibration-and-YoloV5-Recognition/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Project Repo: KinectV2 Camera Calibration and Yolov5 Recognition 2022-12-30 19:11 This is a subproject from camera-position-solution.  BackgroundWe are assigned the mission to combine KinectV2 Camera">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/image_2023-01-04-00-07-18.png">
<meta property="og:image" content="http://example.com/img/image_2022-12-30-19-10-06.jpg">
<meta property="og:image" content="http://example.com/img/image_2022-12-30-19-11-30.png">
<meta property="og:image" content="http://example.com/img/image_2022-12-30-21-19-15.png">
<meta property="og:image" content="http://example.com/img/image_2022-12-31-00-10-18.png">
<meta property="og:image" content="http://example.com/img/image_2022-12-31-00-19-10.png">
<meta property="article:published_time" content="2024-10-15T12:21:14.246Z">
<meta property="article:modified_time" content="2024-10-15T12:21:14.246Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="记录">
<meta property="article:tag" content="KinectV2">
<meta property="article:tag" content="Opencv">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/image_2023-01-04-00-07-18.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2022-12-30-KinectV2-Camera-Calibration-and-YoloV5-Recognition" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/10/15/2022-12-30-KinectV2-Camera-Calibration-and-YoloV5-Recognition/" class="article-date">
  <time class="dt-published" datetime="2024-10-15T12:21:14.246Z" itemprop="datePublished">2024-10-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Post/">Post</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      KinectV2 Camera Calibration and `Yolov5` Recognition
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <ul>
<li>Project Repo: <a target="_blank" rel="noopener" href="https://github.com/ChrisVicky/KinectV2-calibration-and-Yolov5-recognition">KinectV2 Camera Calibration and <code>Yolov5</code> Recognition</a></li>
<li>2022-12-30 19:11</li>
<li>This is a subproject from <a target="_blank" rel="noopener" href="https://github.com/ChrisVicky/camera-position-solution">camera-position-solution</a>.</li>
</ul>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>We are assigned the mission to combine KinectV2 Camera and a Robot car to construct a system that automatically calculate the camera’s position and can tell where some objects are only according to camera’s perspective (Of course here we use Yolov5 to recognize objects);</p>
<center>
  <img style="border-radius: 0.3125em;box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);"
    src="/img/image_2023-01-04-00-07-18.png"><br>
  <div style="color:orange; border-bottom: 1px solid #d9d9d9;display: inline-block;color: #999;padding: 2px;">KinectV2</div>
</center>

<p>In this part, we use the chess board instead of the robot car to accomplish the calibration part and then calculate a perspective transformation matrix that maps points in the image (aka pixel coordinates) to the desk (or bed) coordinates.</p>
<span id="more"></span>

<p><img src="/img/image_2022-12-30-19-10-06.jpg" alt="KinectV2"></p>
<h2 id="Project-Structure"><a href="#Project-Structure" class="headerlink" title="Project Structure"></a>Project Structure</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── build                         -- build Dir</span><br><span class="line">├── CMakeLists.txt                -- Top Cmake Configuration</span><br><span class="line">├── data                          </span><br><span class="line">│   └── output.mp4                -- Output Data -&gt; stacks of imshown frames</span><br><span class="line">├── default.xml                   -- Default Configuration File (example)</span><br><span class="line">├── include </span><br><span class="line">│   ├── calibration.hpp           -- Calibration -&gt; Future change: With Robot</span><br><span class="line">│   ├── define.hpp                -- Define COLORS etc</span><br><span class="line">│   ├── dnn.hpp                   -- Use OpenCV DNN APIs</span><br><span class="line">│   ├── main.hpp                  -- Main Program</span><br><span class="line">│   └── settings.hpp              -- Read Settings</span><br><span class="line">├── logsrc                        -- Log Helper by loguru</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── loguru.cpp</span><br><span class="line">│   └── loguru.hpp</span><br><span class="line">├── models                        -- Trained Yolov5 Modules</span><br><span class="line">│   ├── yolov5n.onnx</span><br><span class="line">│   ├── yolov5s.onnx</span><br><span class="line">│   ├── yolov5.xml</span><br><span class="line">│   └── yuki-bubu-2022-12-23.onnx </span><br><span class="line">├── README.md                     </span><br><span class="line">└── src                           </span><br><span class="line">    ├── calibration.cpp</span><br><span class="line">    ├── dnn.cpp</span><br><span class="line">    └── main.cpp</span><br><span class="line">21 directories, 87 files</span><br></pre></td></tr></table></figure>

<h2 id="Dependency"><a href="#Dependency" class="headerlink" title="Dependency"></a>Dependency</h2><p>  We use two libraries: <a target="_blank" rel="noopener" href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> and <a target="_blank" rel="noopener" href="https://github.com/opencv/opencv">OpenCV</a>.</p>
<blockquote>
<p>Note that: You should set <code>freenect2_DIR</code> and include <code>freenect2_INCLUDE</code> directions if you install <code>libfreenect2</code> in custom directories.</p>
</blockquote>
  <figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up libfreenect2</span></span><br><span class="line"><span class="comment"># Set include dir and DIR for libfreenect2 if it is not installed globally</span></span><br><span class="line"><span class="comment"># SET(freenect2_DIR /home/christopher/Coding/libfreenect2/freenect2/lib/cmake/freenect2)</span></span><br><span class="line"><span class="comment"># include_directories(/home/christopher/Coding/libfreenect2/freenect2/include/)</span></span><br><span class="line"><span class="keyword">find_package</span>(freenect2 REQUIRED)</span><br></pre></td></tr></table></figure>

<h2 id="Program-Usage"><a href="#Program-Usage" class="headerlink" title="Program Usage"></a>Program Usage</h2><ol>
<li><p>Install Dependencies shown above</p>
</li>
<li><p>Run the following commands to build the project</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p build &amp;&amp; cd build</span><br><span class="line">cmake ..</span><br><span class="line">cmake --build .</span><br></pre></td></tr></table></figure></li>
<li><p>Plug in KinectV2 via USB (you might need a hub)<br><img src="/img/image_2022-12-30-19-11-30.png" alt="Hub"></p>
</li>
<li><p>run <code>./calibration</code> to start program<br><img src="/img/image_2022-12-30-21-19-15.png" alt="Screenshot"></p>
</li>
</ol>
<h2 id="Developer-Diary"><a href="#Developer-Diary" class="headerlink" title="Developer Diary"></a>Developer Diary</h2><p>  To meet the need, we have to conquer four difficulties.</p>
<ol>
<li>KinectV2 Connection</li>
<li>OpenCV Calibration</li>
<li>Object Detection</li>
<li>Planes Transformation</li>
</ol>
<h3 id="1-KinectV2-Connection"><a href="#1-KinectV2-Connection" class="headerlink" title="1. KinectV2 Connection"></a>1. KinectV2 Connection</h3><p>Since we develop the program on multiple Operating Systems (OS), we decide to take advantage of <a target="_blank" rel="noopener" href="https://github.com/OpenKinect/libfreenect2">Libfreenect2</a> which is open-sourced and supports Linux, Windows and Mac-OS. </p>
<blockquote>
<p>To install Libfreenect2, we simply go through the steps described on the README page of the project. Note that I’m running the Arch Linux with 6.0.12 Linux Kernel at the time of this post, and the lib works fine.</p>
</blockquote>
<p>To use KinectV2, we need the following steps:</p>
<h4 id="1-1-Define-Basic-Variables-either-globally-or-locally"><a href="#1-1-Define-Basic-Variables-either-globally-or-locally" class="headerlink" title="1.1. Define Basic Variables, either globally or locally."></a>1.1. Define Basic Variables, either globally or locally.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">libfreenect2::Freenect2 freenect2;      <span class="comment">// libfreenect2 entity</span></span><br><span class="line">libfreenect2::PacketPipeline *pipeline; <span class="comment">// libfreenect2 pipeline</span></span><br><span class="line">libfreenect2::Freenect2Device *device;  <span class="comment">// device</span></span><br><span class="line"><span class="function">libfreenect2::SyncMultiFrameListener <span class="title">listener</span><span class="params">(libfreenect2::Frame::Color)</span></span>;</span><br><span class="line">libfreenect2::FrameMap frames;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-Initialize-the-device-via-certain-APIs"><a href="#1-2-Initialize-the-device-via-certain-APIs" class="headerlink" title="1.2. Initialize the device via certain APIs"></a>1.2. Initialize the <code>device</code> via certain APIs</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* -------------------- START Kinectv2 Initialization -------------------- */</span></span><br><span class="line"><span class="keyword">if</span>(freenect<span class="number">2.</span><span class="built_in">enumerateDevices</span>() == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;no device connected!&quot;</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device connected&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">string serial = freenect<span class="number">2.</span><span class="built_in">getDefaultDeviceSerialNumber</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;SEARIAL Number: %s&quot;</span>,serial.<span class="built_in">c_str</span>());</span><br><span class="line">pipeline = <span class="keyword">new</span> libfreenect2::<span class="built_in">CpuPacketPipeline</span>();</span><br><span class="line">device = freenect<span class="number">2.</span><span class="built_in">openDevice</span>(serial, pipeline); <span class="keyword">if</span>(device == <span class="number">0</span>)&#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(ERROR, <span class="string">&quot;failed to open device: %s&quot;</span>, serial.<span class="built_in">c_str</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device opened&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">kinect_shutdown = <span class="literal">false</span>;</span><br><span class="line">device-&gt;<span class="built_in">setColorFrameListener</span>(&amp;listener);</span><br><span class="line">device-&gt;<span class="built_in">start</span>();</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device serial: %s&quot;</span> ,device-&gt;<span class="built_in">getSerialNumber</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="built_in">LOG_F</span>(INFO, <span class="string">&quot;device firmware: %s&quot;</span> ,device-&gt;<span class="built_in">getFirmwareVersion</span>().<span class="built_in">c_str</span>());</span><br><span class="line"><span class="comment">/* -------------------- END Kinectv2 Initialization -------------------- */</span></span><br></pre></td></tr></table></figure>

<h4 id="1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device"><a href="#1-3-We-shall-start-a-Loop-to-receive-frames-from-the-Device" class="headerlink" title="1.3. We shall start a Loop to receive frames from the Device"></a>1.3. We shall start a Loop to receive frames from the Device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(!kinect_shutdown)&#123;</span><br><span class="line">  <span class="keyword">if</span>(!listener.<span class="built_in">waitForNewFrame</span>(frames, timeout))</span><br><span class="line">    <span class="built_in">LOG_F</span>(WARNING, <span class="string">&quot;Frame Received Failed after timeout: %d&quot;</span>, timeout);</span><br><span class="line">  libfreenect2::Frame *rgb = frames[libfreenect2::Frame::Color];</span><br><span class="line">  <span class="comment">/* -------------------- START Frame Processing -------------------- */</span></span><br><span class="line">  <span class="comment">/* -------------------- END Frame Processing -------------------- */</span></span><br><span class="line">  listener.<span class="built_in">release</span>(frames);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device"><a href="#1-4-Before-Exit-we-need-to-manually-stop-and-close-the-device" class="headerlink" title="1.4. Before Exit, we need to manually stop and close the device"></a>1.4. Before Exit, we need to manually stop and close the device</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">device-&gt;<span class="built_in">close</span>();</span><br></pre></td></tr></table></figure>
<blockquote>
<p>We must define a <code>sigint_handler</code> to handle crash-down exit, or the device just go on pushing frames to stack via USB and never stops until the computer shutdown.</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">sigint_handler</span><span class="params">(<span class="type">int</span> s)</span></span>&#123;</span><br><span class="line">  device-&gt;<span class="built_in">stop</span>();</span><br><span class="line">  device-&gt;<span class="built_in">close</span>();</span><br><span class="line">  <span class="built_in">exit</span>(s);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Usage </span></span><br><span class="line"><span class="built_in">signal</span>(SIGINT, sigint_handler); <span class="comment">// Savely Close the Device before sudden exit</span></span><br></pre></td></tr></table></figure>

<h4 id="1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing"><a href="#1-5-To-take-advantage-of-OpenCV-APIs-we-convert-libfreenect2-Frame-to-cv-Mat-right-at-the-beginning-of-Frame-Processing" class="headerlink" title="1.5. To take advantage of OpenCV APIs, we convert libfreenect2::Frame to cv::Mat right at the beginning of Frame Processing."></a>1.5. To take advantage of OpenCV APIs, we convert <code>libfreenect2::Frame</code> to <code>cv::Mat</code> right at the beginning of <code>Frame Processing</code>.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cv::<span class="built_in">Mat</span>(rgb-&gt;height, rgb-&gt;width, CV_8UC4, rgb-&gt;data).<span class="built_in">copyTo</span>(kinect_mat);</span><br><span class="line">cv::<span class="built_in">flip</span>(kinect_mat, kinect_mat, <span class="number">1</span>);</span><br><span class="line">rgb_mat = cv::Mat::<span class="built_in">zeros</span>(kinect_mat.<span class="built_in">size</span>(),CV_8UC3);</span><br><span class="line"><span class="built_in">mixChannels</span>(kinect_mat, rgb_mat, &#123;<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: <code>libfreenect2::Frame</code> contains 4 channels while our yolov5 model takes only 3-channel inputs, so we perform a <code>mixChannels()</code> here to reduce the last one.</p>
</blockquote>
<hr>
<h3 id="2-OpenCV-Calibration"><a href="#2-OpenCV-Calibration" class="headerlink" title="2. OpenCV Calibration"></a>2. OpenCV Calibration</h3><p>To be more specified, in our original plan, the robot car, armed with SLAM, would provide information in 3D-world-coordinate-system while the KinectV2 camera shall recognize the car via some sort of object-recognition technic (e.g. <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YoloV5</a>) and provides its position in 2D-pixel-coordinate-system. Timestamp enables us to match them up, forming a set of 2D-3D points pair. Therefore, the problem turns into a <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">Perspective-n-Point(aka <code>PnP</code>)</a> problem, and it has been solved long ago. OpenCV provides multiple APIs that implement nearly every solution posted literally.</p>
<p>However, because of the COVID-19 lockdown, I was separated from my teammates and I only have the KinetV2 camera by hand. Thus, I use built-in calibration functionality with chessboard to obtain the set of 2D-3D points pair to accomplish the task.</p>
<p>We perform 4 steps to meet the need.</p>
<h4 id="2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still"><a href="#2-1-Collect-multiple-frames-where-the-camera-and-chessboard-are-relatively-still" class="headerlink" title="2.1. Collect multiple frames where the camera and chessboard are relatively still."></a>2.1. Collect multiple frames where the camera and chessboard are relatively still.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(STATE == STATE_START_CALIBRATION)</span><br><span class="line">  cali_frames.<span class="built_in">push_back</span>(rgb_mat);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: We use <code>STATE</code> to control the program. In fact, the whole program is designed on a Finite-State Machine(FSM).</p>
</blockquote>
<h4 id="2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system"><a href="#2-2-Run-findPattern-to-obtain-feature-points’-position-in-2D-pixel-coordinate-system" class="headerlink" title="2.2. Run findPattern to obtain feature points’ position in 2D-pixel-coordinate-system."></a>2.2. Run <code>findPattern</code> to obtain feature points’ position in 2D-pixel-coordinate-system.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; point_buff;</span><br><span class="line"><span class="type">int</span> board_flag = CALIB_CB_ADAPTIVE_THRESH | CALIB_CB_NORMALIZE_IMAGE | CALIB_CB_FAST_CHECK;</span><br><span class="line"><span class="type">int</span> found = <span class="built_in">findChessboardCorners</span>(rgb_mat, boardSize, point_buff, board_flag);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: We use <code>PThread</code> to accelerate the process, finding patterns in all collected frames at once.</p>
</blockquote>
<h4 id="2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information"><a href="#2-3-Collect-all-2D-information-and-calculate-3D-world-coordinate-system-information" class="headerlink" title="2.3. Collect all 2D information and calculate 3D-world-coordinate-system information."></a>2.3. Collect all 2D information and calculate 3D-world-coordinate-system information.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> found=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;size;++i)&#123;</span><br><span class="line">  <span class="type">void</span> * ret;</span><br><span class="line">  <span class="built_in">pthread_join</span>(thread_ids[i], &amp;ret);</span><br><span class="line">  runCalibrationRet retVal = *(runCalibrationRet*) ret;</span><br><span class="line">  <span class="keyword">if</span>(retVal.found)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!found)</span><br><span class="line">      d2s = retVal.d2;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;d2s.<span class="built_in">size</span>();j++) d2s[j] += retVal.d2[j];</span><br><span class="line">    found ++;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;boardSize.height; ++i)</span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;boardSize.width; ++j)</span><br><span class="line">    d3s.<span class="built_in">push_back</span>(<span class="built_in">Point3f</span>(j*squareSize, i*squareSize, <span class="number">0</span>));</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: 3D-world-coordinate-system information is defined manually. The chessboard is the perfect coordinate system.</p>
</blockquote>
<h4 id="2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec"><a href="#2-4-Wrap-them-up-and-perform-solvePnP-to-get-rvec-and-tvec" class="headerlink" title="2.4. Wrap them up and perform solvePnP to get rvec and tvec"></a>2.4. Wrap them up and perform <code>solvePnP</code> to get <code>rvec</code> and <code>tvec</code></h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">solvePnP</span>(d3s, d2s, camera_matrix, dist_coeffs, rvec, tvec);</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that:<code>camera_matrix</code> and <code>dist_coeffs</code> are both ‘known’ parameters. They can be obtained either through manufacturer or calibrated by programs. OpenCV provides one API and with a little patch shall we be able to calibrate it.</p>
</blockquote>
<h4 id="2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained"><a href="#2-5-To-obtain-camera-position-we-still-need-another-step-that-takes-both-rvec-and-tvec-as-input-and-camera-position-would-be-obtained" class="headerlink" title="2.5. To obtain camera position, we still need another step that takes both rvec and tvec as input and camera_position would be obtained."></a>2.5. To obtain camera position, we still need another step that takes both <code>rvec</code> and <code>tvec</code> as input and <code>camera_position</code> would be obtained.</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> type_tv = tv.<span class="built_in">type</span>();</span><br><span class="line"><span class="function">Mat <span class="title">rvf</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="comment">// Convert from vector rv(3x1) to matrix rotation(3x3)</span></span><br><span class="line"><span class="built_in">Rodrigues</span>(rv, rvf);</span><br><span class="line"><span class="comment">// The Inversed Matrix</span></span><br><span class="line"><span class="function">Mat <span class="title">rvf_1</span><span class="params">(<span class="number">3</span>,<span class="number">3</span>,type_tv)</span></span>;</span><br><span class="line"><span class="built_in">invert</span>(rvf, rvf_1, DECOMP_SVD);</span><br><span class="line">Mat Position = rvf_1 * (-tv);</span><br><span class="line"><span class="function">Point3f <span class="title">p</span><span class="params">(Position)</span></span>;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: In Computer Vision, there are four basic coordinate systems and here we use the ‘extrinsic’ matrix, which converts between 3D-world-coordinate-system and 3D-camera-coordinate-system, to calculate camera position. For detail: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html">OpenCV <code>Pnp</code> reference</a>.</p>
</blockquote>
<blockquote>
<p>Also note that: <code>Rodrigues</code> is essential, the output, <code>rvec</code> is (3x1), reference: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac">Rodrigues</a></p>
</blockquote>
<hr>
<h3 id="3-Object-Detection"><a href="#3-Object-Detection" class="headerlink" title="3. Object Detection"></a>3. Object Detection</h3><p>According to our original plan, the KinectV2 Camera should recognize the Robot car in order to obtain its position in 2D-pixel-coordinate-system. So I perform a test with the famous <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YoloV5 Project</a>.</p>
<p>The hardest part here is not the training part, YoloV5 provides a rather simple API to format data and train it on pre-trained models (reference: <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">Train on Custom Data</a>). The hardest part is to mix the model in C++ program. After some research, I find that OpenCV provides APIs <code>cv::dnn</code> that load <code>.onnx</code> models and can run forward actions, or deduction.</p>
<p>In this part, I use my two cats as dataset. In the following steps, I would demonstrate the way to set up datasets, train model and use the model via OpenCV APIs.</p>
<h4 id="3-1-Dataset-Creation"><a href="#3-1-Dataset-Creation" class="headerlink" title="3.1. Dataset Creation"></a>3.1. Dataset Creation</h4><p>Simply follow steps described on <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data">this page</a>. The output should be similar as below:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── data.yaml</span><br><span class="line">├── README.dataset.txt</span><br><span class="line">├── README.roboflow.txt</span><br><span class="line">├── test</span><br><span class="line">├── train</span><br><span class="line">└── valid</span><br><span class="line">9 directories, 382 files</span><br></pre></td></tr></table></figure>

<h4 id="3-2-Train-model"><a href="#3-2-Train-model" class="headerlink" title="3.2 Train model"></a>3.2 Train model</h4><p>Simply put datasets in yolov5 directory and perform the following command and sit back to wait for the results</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python train.py --img 640 --batch 16 --epochs 3 --data data.yaml --weights yolov5s.pt</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Note that: You should clone yolov5 repo before training and of course set up python environment. </p>
</blockquote>
<h4 id="3-3-Convert-model-to-onnx"><a href="#3-3-Convert-model-to-onnx" class="headerlink" title="3.3 Convert model to .onnx"></a>3.3 Convert model to <code>.onnx</code></h4><p>YoloV5 takes PyTorch as backend, thus, the models are saved as <code>.pt</code> format. However, <code>cv::dnn</code> prefers <code>.onnx</code> format. Thus, a conversion shall be performed.</p>
<p>At this point (2022-12-30), the transformation based on the default dependency of YoloV5 is not compatible with the latest version of OpenCV <code>dnn</code> module. I have posted an <a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5/issues/10575">Issue</a> on this to YoloV5 and get the information that it is the OpenCV that can not decode the model. Somehow, I manage to conquer the issue by downgrading some essential packages. My anaconda environment configuration is uploaded within the project.</p>
<p>After the correction of Dependency, we perform the following command to export <code>.onnx</code> from <code>.pt</code>.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python export.py --weights yolov5s.pt --include onnx</span><br></pre></td></tr></table></figure>

<h4 id="3-4-Load-onnx-with-OpenCV"><a href="#3-4-Load-onnx-with-OpenCV" class="headerlink" title="3.4 Load .onnx with OpenCV"></a>3.4 Load <code>.onnx</code> with OpenCV</h4><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv::dnn::Net net = cv::dnn::<span class="built_in">readNetFromONNX</span>(model_path);</span><br><span class="line">net.<span class="built_in">setPreferableBackend</span>(cv::dnn::DNN_BACKEND_OPENCV);</span><br><span class="line">net.<span class="built_in">setPreferableTarget</span>(cv::dnn::DNN_TARGET_CPU);</span><br></pre></td></tr></table></figure>

<h4 id="3-5-Format-Input-Data"><a href="#3-5-Format-Input-Data" class="headerlink" title="3.5 Format Input Data"></a>3.5 Format Input Data</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cv::Mat blob;</span><br><span class="line">int col = frame.cols;</span><br><span class="line">int row = frame.rows;</span><br><span class="line">int _max = max(col, row);</span><br><span class="line">input_img = cv::Mat::zeros(_max, _max, CV_8UC3);</span><br><span class="line">frame.copyTo(input_img(cv::Rect(0, 0, col, row)));</span><br><span class="line">cv::dnn::blobFromImage(input_img, blob, 1./255., cv::Size(INPUT_WIDTH, INPUT_HEIGHT), cv::Scalar(), true, false);</span><br></pre></td></tr></table></figure>

<h4 id="3-6-Forward-Network"><a href="#3-6-Forward-Network" class="headerlink" title="3.6 Forward Network"></a>3.6 Forward Network</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net.setInput(input);</span><br><span class="line">std::vector&lt;cv::Mat&gt; outputs;</span><br><span class="line">net.forward(outputs, net.getUnconnectedOutLayersNames());</span><br></pre></td></tr></table></figure>

<h4 id="3-7-Format-the-output"><a href="#3-7-Format-the-output" class="headerlink" title="3.7 Format the output"></a>3.7 Format the output</h4><p>The output of the model, as the result of network-forwarding, is defined as follows:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> structure of `output.data`</span><br><span class="line">┌─┬─┬─┬─┬─┬────────┬─────────────────────►</span><br><span class="line">│0│1│2│3│4│5 ......│dimensions</span><br><span class="line">├─┼─┼─┼─┼─┼────────┼─┬─┬─┬─┬─┬────────┬──►</span><br><span class="line">│x│y│w│h│c│[scores]│x│y│w│h│c│[scores]│..</span><br><span class="line">└─┴─┴─┴─┴─┴────────┴─┴─┴─┴─┴─┴────────┴──►</span><br><span class="line"> c: confidence</span><br></pre></td></tr></table></figure>
<p>Basically, it is an array that can be accessed via its address. The code is too large to be shown here.</p>
<h4 id="3-8-Transfer-Output-to-Detection"><a href="#3-8-Transfer-Output-to-Detection" class="headerlink" title="3.8 Transfer Output to Detection"></a>3.8 Transfer Output to <code>Detection</code></h4><p>For easy access, we transfer the output to the following data format.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Detection&#123;</span><br><span class="line">	int       class_id;   // Result&#x27;s class id</span><br><span class="line">	float     confidence; // Probability</span><br><span class="line">	cv::Rect  box;        // Where it is</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h4 id="3-9-Draw-Boxes-around-Targets"><a href="#3-9-Draw-Boxes-around-Targets" class="headerlink" title="3.9 Draw Boxes around Targets"></a>3.9 Draw Boxes around Targets</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">int detection_size = output.size();</span><br><span class="line">for(int i=0;i&lt;detection_size;++i)&#123;</span><br><span class="line">	auto detection = output[i];</span><br><span class="line">	auto box = detection.box;</span><br><span class="line">	auto class_id = detection.class_id;</span><br><span class="line">	const auto color = color_list[class_id%color_list.size()];</span><br><span class="line">	cv::rectangle(frame, box, color, 2);</span><br><span class="line">	cv::rectangle(frame, cv::Point(box.x, box.y - 20), cv::Point(box.x + box.width, box.y), color, cv::FILLED);</span><br><span class="line">	cv::putText(frame, cv::format(&quot;%s: %.3f&quot;,s.classifications[class_id].c_str(),detection.confidence), cv::Point(box.x, box.y - 5), cv::FONT_HERSHEY_COMPLEX, 0.6, BLACK);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="4-Plane-Transformation"><a href="#4-Plane-Transformation" class="headerlink" title="4. Plane Transformation"></a>4. Plane Transformation</h3><p>Finally, we convert any points on the 2D-pixel-coordinate-system to its position in the 3D-world-coordinate-system. However, according some hard math, it is not possible to convert 2D to 3D without a given plane. Shown below, here is a model of Computer Vision (Reference: <a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Display-of-Various-Coordinate-Systems-for-a-Computer-Vision-System-i-i-i-i-1-i_fig2_337311806">ResearchGate</a>)</p>
<p><img src="/img/image_2022-12-31-00-10-18.png" alt="Computer Vision Model"></p>
<p>Take the example of the transformation of Point $^1p_1$. The 3D position of it can be anywhere on the line of $O\ ^1p_1$, not necessary be at Point $^wp_1$ unless we require the 3D position lies on a particular plane.</p>
<p>Thus, in our case, we explicitly define that the Z axis of the object must be 0, meaning that we only provide the position of it on the ground $XOY$ axis and not provide the height.</p>
<p>Then, the problem is simplified to calculate a transformation between two planes. Here we take advantage of another API by OpenCV: <a target="_blank" rel="noopener" href="https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#ga20f62aa3235d869c9956436c870893ae"><code>getPerspectiveTransformation</code></a>. According to the definition below, the input <code>src</code> and <code>dst</code> must be vertices of a quadrangle. And the return value shall be the transformation matrix.</p>
<p><img src="/img/image_2022-12-31-00-19-10.png" alt="`getPerspectiveTransform` definition"></p>
<h4 id="4-1-Calculate-Transformation-Matrix"><a href="#4-1-Calculate-Transformation-Matrix" class="headerlink" title="4.1 Calculate Transformation Matrix"></a>4.1 Calculate Transformation Matrix</h4><p>In our project, we take 4 vertices of the chessboard to be the input.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; desk;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> d3:d3s) desk.<span class="built_in">push_back</span>(<span class="built_in">Point2f</span>(d<span class="number">3.</span>x,d<span class="number">3.</span>y));</span><br><span class="line">Point2f in[<span class="number">4</span>];</span><br><span class="line">Point2f out[<span class="number">4</span>];</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> helper(_in, in_) \</span></span><br><span class="line"><span class="meta">_in[0] = in_[i0]; \</span></span><br><span class="line"><span class="meta">_in[1] = in_[i1]; \</span></span><br><span class="line"><span class="meta">_in[2] = in_[i2]; \</span></span><br><span class="line"><span class="meta">_in[3] = in_[i3]</span></span><br><span class="line"><span class="type">int</span> i0 = <span class="number">0</span>, i1 = boardSize.width<span class="number">-1</span>;</span><br><span class="line"><span class="type">int</span> i2 = boardSize.width * (boardSize.height<span class="number">-1</span>);</span><br><span class="line"><span class="type">int</span> i3 = boardSize.width * boardSize.height - <span class="number">1</span>;</span><br><span class="line"><span class="built_in">helper</span>(in, d2s);</span><br><span class="line"><span class="built_in">helper</span>(out, desk);</span><br><span class="line"><span class="meta">#<span class="keyword">undef</span> helper</span></span><br><span class="line"><span class="comment">// <span class="doctag">NOTE:</span> According to reference (Opencv), </span></span><br><span class="line"><span class="comment">// getPerspectiveTransform takes quadrangle vertices in the source image</span></span><br><span class="line">pix23D = <span class="built_in">getPerspectiveTransform</span>(in, out);</span><br></pre></td></tr></table></figure>

<h4 id="4-2-Calculate-Corresponding-3D-Position"><a href="#4-2-Calculate-Corresponding-3D-Position" class="headerlink" title="4.2 Calculate Corresponding 3D Position"></a>4.2 Calculate Corresponding 3D Position</h4><p><code>perspectiveTransformation</code> have already done for us.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;Point2f&gt; out;</span><br><span class="line">vector&lt;Point2f&gt; in; in.push_back(p2);</span><br><span class="line">cv::perspectiveTransform(in, out, pix23D);</span><br><span class="line">Point3f ret = Point3f(out[0].x,out[0].y,0);</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Till now, the Project is half-way finished and seems pretty simple, only taking advantage of existing Methods, APIs and Models. In the next semester, we would combine the robot car to accomplish the original plan. </p>
<p>The next aim of our project would be human-skeleton detection and action deduction with it. And finally, adding some WiFi-Sensor Information would enable us to build a more robust and more complete in-home monitor system.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/10/15/2022-12-30-KinectV2-Camera-Calibration-and-YoloV5-Recognition/" data-id="cm2aewu5a000vpcjvgxrmdnqb" data-title="KinectV2 Camera Calibration and `Yolov5` Recognition" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/KinectV2/" rel="tag">KinectV2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Opencv/" rel="tag">Opencv</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%B0%E5%BD%95/" rel="tag">记录</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/10/15/2022-11-15-deploy-hadoop/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Deploy Hadoop with VirtualBox</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Diary/">Diary</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Note/">Note</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/PaperReview/">PaperReview</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Post/">Post</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Big-Data/" rel="tag">Big Data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Clash/" rel="tag">Clash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DIY/" rel="tag">DIY</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hadoop/" rel="tag">Hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hardware/" rel="tag">Hardware</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KinectV2/" rel="tag">KinectV2</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ML/" rel="tag">ML</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/OpenPose/" rel="tag">OpenPose</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Opencv/" rel="tag">Opencv</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PAF/" rel="tag">PAF</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PaperReview/" rel="tag">PaperReview</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Performance-Improve/" rel="tag">Performance Improve</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pose-Estimation/" rel="tag">Pose Estimation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Systemd/" rel="tag">Systemd</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TJU/" rel="tag">TJU</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tranformer/" rel="tag">Tranformer</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VPN/" rel="tag">VPN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ViT/" rel="tag">ViT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/learn/" rel="tag">learn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/playwright/" rel="tag">playwright</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/" rel="tag">ssh</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag">学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" rel="tag">数据结构</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" rel="tag">期末复习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AA%B2%E7%A8%8B%E4%BD%9C%E6%A5%AD/" rel="tag">課程作業</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AE%B0%E5%BD%95/" rel="tag">记录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%AF%84%E6%95%99/" rel="tag">评教</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/AI/" style="font-size: 15px;">AI</a> <a href="/tags/Big-Data/" style="font-size: 10px;">Big Data</a> <a href="/tags/Clash/" style="font-size: 10px;">Clash</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Hadoop/" style="font-size: 10px;">Hadoop</a> <a href="/tags/Hardware/" style="font-size: 10px;">Hardware</a> <a href="/tags/KinectV2/" style="font-size: 10px;">KinectV2</a> <a href="/tags/Linux/" style="font-size: 20px;">Linux</a> <a href="/tags/ML/" style="font-size: 10px;">ML</a> <a href="/tags/OpenPose/" style="font-size: 10px;">OpenPose</a> <a href="/tags/Opencv/" style="font-size: 10px;">Opencv</a> <a href="/tags/PAF/" style="font-size: 10px;">PAF</a> <a href="/tags/PaperReview/" style="font-size: 10px;">PaperReview</a> <a href="/tags/Performance-Improve/" style="font-size: 10px;">Performance Improve</a> <a href="/tags/Pose-Estimation/" style="font-size: 10px;">Pose Estimation</a> <a href="/tags/Systemd/" style="font-size: 10px;">Systemd</a> <a href="/tags/TJU/" style="font-size: 10px;">TJU</a> <a href="/tags/Tranformer/" style="font-size: 10px;">Tranformer</a> <a href="/tags/VPN/" style="font-size: 10px;">VPN</a> <a href="/tags/ViT/" style="font-size: 10px;">ViT</a> <a href="/tags/learn/" style="font-size: 15px;">learn</a> <a href="/tags/playwright/" style="font-size: 10px;">playwright</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/%E5%AD%A6%E4%B9%A0/" style="font-size: 10px;">学习</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" style="font-size: 10px;">数据结构</a> <a href="/tags/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" style="font-size: 10px;">期末复习</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">笔记</a> <a href="/tags/%E8%AA%B2%E7%A8%8B%E4%BD%9C%E6%A5%AD/" style="font-size: 10px;">課程作業</a> <a href="/tags/%E8%AE%B0%E5%BD%95/" style="font-size: 15px;">记录</a> <a href="/tags/%E8%AF%84%E6%95%99/" style="font-size: 10px;">评教</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">September 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">February 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/10/15/2022-12-30-KinectV2-Camera-Calibration-and-YoloV5-Recognition/">KinectV2 Camera Calibration and `Yolov5` Recognition</a>
          </li>
        
          <li>
            <a href="/2024/10/15/2022-11-15-deploy-hadoop/">Deploy Hadoop with VirtualBox</a>
          </li>
        
          <li>
            <a href="/2024/10/15/2022-11-18-Add-Extra-Memory-to-Thinkpad-T14/">Add Extra Memory to Thinkpad T14</a>
          </li>
        
          <li>
            <a href="/2024/10/15/2022-12-12-Source-Backup-for-Disk-Update/">Source Backup for Disk Update</a>
          </li>
        
          <li>
            <a href="/2023/09/25/2023-09-25-Github-Image-Bed/">Github Image Bed</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>